{
    "activation":	"lrelu",
    "alpha":	"auto",
    "batch_size":	128,
    "experiment_id":	"CON-76077",
    "gamma":	0.99,
    "hidden_sizes":	[
        256,
        256,
        256,
        256
    ],
    "log_every":	20000,
    "lr":	0.001,
    "replay_size":	1000000,
    "run_kind":	"single",
    "seed":	15,
    "steps":	1000000,
    "target_output_std":	0.089,
    "task":	"push-wall-v1",
    "use_layer_norm":	true
}